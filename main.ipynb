{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngOaL4uFUZbj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "GITHUB_USERNAME = \"MUKAMAFrancois\"\n",
    "REPO_NAME = \"TTS_tecGrw\"\n",
    "GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
    "project_dir = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "if os.path.exists(project_dir):\n",
    "    print(\"Updating project repo...\")\n",
    "    %cd {project_dir}\n",
    "    !git pull\n",
    "else:\n",
    "    print(\"Cloning project repo...\")\n",
    "    repo_url = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "    !git clone {repo_url} {project_dir}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-KxuhmeUZbl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"/content/{REPO_NAME}\")\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3jvkEn0UZbm"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.56,<5\" \"tokenizers>=0.22,<0.24\" accelerate tqdm torchaudio datasets pyyaml pandas soundfile speechbrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f03F21fpUZbm"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.loader import load_config, get_hf_token\n",
    "from src.preprocess import run_preprocessing_pipeline\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "\n",
    "def resolve_path(cfg, key):\n",
    "    raw = cfg.get(key)\n",
    "    if raw is None:\n",
    "        return None\n",
    "    p = Path(raw)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    p2 = Path(cfg.get(\"PROCESSED_DIR\", \"\")) / p.name\n",
    "    return p2\n",
    "\n",
    "\n",
    "def metadata_non_empty(path_obj):\n",
    "    return path_obj is not None and path_obj.exists() and path_obj.stat().st_size > 0\n",
    "\n",
    "\n",
    "train_meta = resolve_path(config, \"TRAIN_METADATA\")\n",
    "val_meta = resolve_path(config, \"VAL_METADATA\")\n",
    "\n",
    "if metadata_non_empty(train_meta) and metadata_non_empty(val_meta):\n",
    "    print(f\"Processed metadata found: {train_meta} and {val_meta}\")\n",
    "else:\n",
    "    print(\"Processed metadata missing or empty. Running preprocessing pipeline...\")\n",
    "    token = get_hf_token()\n",
    "    stats = run_preprocessing_pipeline(config, token)\n",
    "    print(\"Preprocessing stats:\", stats)\n",
    "\n",
    "train_meta = resolve_path(config, \"TRAIN_METADATA\")\n",
    "val_meta = resolve_path(config, \"VAL_METADATA\")\n",
    "print(\"TRAIN_METADATA:\", train_meta)\n",
    "print(\"VAL_METADATA:\", val_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HIbvcBWUZbn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.training.speecht5_pipeline import (\n",
    "    TTSDataCollatorWithPadding,\n",
    "    build_processed_datasets,\n",
    "    get_speaker_embedding,\n",
    "    load_speecht5_components,\n",
    "    load_train_val_datasets,\n",
    "    print_preprocessed_batch_debug,\n",
    ")\n",
    "\n",
    "config = load_config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_ds, val_ds = load_train_val_datasets(config)\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples:\", len(val_ds))\n",
    "\n",
    "processor, model, vocoder = load_speecht5_components(device)\n",
    "speaker_embedding = get_speaker_embedding(train_ds, device)\n",
    "\n",
    "train_proc, val_proc = build_processed_datasets(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    speaker_embedding=speaker_embedding,\n",
    ")\n",
    "\n",
    "data_collator = TTSDataCollatorWithPadding(processor, model)\n",
    "print_preprocessed_batch_debug(train_proc, data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggaUhl2yUZbn"
   },
   "outputs": [],
   "source": [
    "from src.training.speecht5_pipeline import build_trainer_bundle, run_stagewise_training\n",
    "\n",
    "bundle = build_trainer_bundle(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_proc=train_proc,\n",
    "    val_proc=val_proc,\n",
    "    data_collator=data_collator,\n",
    "    config=config,\n",
    "    output_dir=\"speecht5_finetuned\",\n",
    ")\n",
    "output_dir = bundle.output_dir\n",
    "trainer = bundle.trainer\n",
    "\n",
    "print(\"TrainingArguments metric_for_best_model:\", trainer.args.metric_for_best_model)\n",
    "print(\"TrainingArguments load_best_model_at_end:\", trainer.args.load_best_model_at_end)\n",
    "print(\"TrainingArguments save_strategy:\", trainer.args.save_strategy)\n",
    "eval_attr = \"eval_strategy\" if hasattr(trainer.args, \"eval_strategy\") else \"evaluation_strategy\"\n",
    "print(f\"TrainingArguments {eval_attr}:\", getattr(trainer.args, eval_attr))\n",
    "print(\"Running a pre-train evaluate() sanity check...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval metrics keys:\", sorted(eval_metrics.keys()))\n",
    "\n",
    "metric_name = trainer.args.metric_for_best_model\n",
    "metric_key = metric_name if metric_name.startswith(\"eval_\") else f\"eval_{metric_name}\"\n",
    "if metric_key not in eval_metrics:\n",
    "    print(f\"WARNING: '{metric_key}' missing from eval metrics. Disabling load_best_model_at_end for this run.\")\n",
    "    trainer.args.load_best_model_at_end = False\n",
    "    trainer.args.metric_for_best_model = None\n",
    "\n",
    "train_result = run_stagewise_training(bundle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q3Dufq6UZbo"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Audio, display\n",
    "from src.training.speecht5_inference import (\n",
    "    configure_generation_for_latency,\n",
    "    export_int8_deployment_package,\n",
    "    export_final_model_package,\n",
    "    get_directory_size_mb,\n",
    "    load_finetuned_model,\n",
    "    save_generation_config,\n",
    "    synthesize_test_sentences,\n",
    ")\n",
    "\n",
    "required_sentences = [\n",
    "    \"Muraho, nagufasha gute uyu munsi?\",\n",
    "    \"Niba ufite ibibazo bijyanye n'ubuzima bwawe, twagufasha.\",\n",
    "    \"Ni ngombwa ko ubonana umuganga vuba.\",\n",
    "    \"Twabanye nawe kandi tuzakomeza kukwitaho.\",\n",
    "    \"Ushobora kuduhamagara igihe cyose ukeneye ubufasha.\",\n",
    "]\n",
    "\n",
    "best_ckpt = trainer.state.best_model_checkpoint or output_dir\n",
    "print(\"Using checkpoint:\", best_ckpt)\n",
    "finetuned_model = load_finetuned_model(best_ckpt, device)\n",
    "configure_generation_for_latency(finetuned_model, max_length=600)\n",
    "save_generation_config(finetuned_model, best_ckpt)\n",
    "\n",
    "audio_paths = synthesize_test_sentences(\n",
    "    model=finetuned_model,\n",
    "    processor=processor,\n",
    "    vocoder=vocoder,\n",
    "    speaker_embedding=speaker_embedding,\n",
    "    sentences=required_sentences,\n",
    "    output_dir=\"evaluation/final_required_sentences\",\n",
    "    device=device,\n",
    "    sample_rate=16000,\n",
    "    fast_maxlenratio=9.0,\n",
    "    safe_maxlenratio=14.0,\n",
    "    retry_for_completeness=True,\n",
    ")\n",
    "\n",
    "for text, audio_path in zip(required_sentences, audio_paths):\n",
    "    print(\"Saved:\", audio_path)\n",
    "    print(\"Text:\", text)\n",
    "    display(Audio(filename=str(audio_path), autoplay=False))\n",
    "\n",
    "int8_dir = export_int8_deployment_package(\n",
    "    finetuned_model,\n",
    "    processor,\n",
    "    \"speecht5_int8_deployment\",\n",
    ")\n",
    "int8_size_mb = get_directory_size_mb(int8_dir)\n",
    "\n",
    "print(f\"\\nINT8 deployment package size: {int8_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AC8rfN6XUZbo"
   },
   "outputs": [],
   "source": [
    "from src.training.speecht5_inference import measure_latency\n",
    "\n",
    "print(\"Inference device:\", device)\n",
    "if getattr(device, \"type\", \"cpu\") != \"cuda\":\n",
    "    print(\"WARNING: running on CPU will usually exceed the 800 ms latency target.\")\n",
    "\n",
    "latencies_ms, mean_ms = measure_latency(\n",
    "    model=finetuned_model,\n",
    "    processor=processor,\n",
    "    vocoder=vocoder,\n",
    "    speaker_embedding=speaker_embedding,\n",
    "    sentences=required_sentences,\n",
    "    device=device,\n",
    "    warmup_runs=1,\n",
    "    maxlenratio=9.0,\n",
    "    threshold=0.50,\n",
    ")\n",
    "\n",
    "for ms, text in zip(latencies_ms, required_sentences):\n",
    "    print(f\"Latency: {ms:.2f} ms | {text}\")\n",
    "\n",
    "print(f\"Mean latency: {mean_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def directory_size_mb(path):\n",
    "    p = Path(path)\n",
    "    total = sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
    "    return total / (1024.0 * 1024.0)\n",
    "\n",
    "# Export clean FP32 inference package\n",
    "fp32_dir = Path(\"speecht5_fp32_infer\")\n",
    "if fp32_dir.exists():\n",
    "    shutil.rmtree(fp32_dir)\n",
    "export_final_model_package(finetuned_model, processor, fp32_dir)\n",
    "fp32_zip = shutil.make_archive(\"speecht5_fp32_infer\", \"zip\", root_dir=str(fp32_dir))\n",
    "fp32_dir_size_mb = directory_size_mb(fp32_dir)\n",
    "fp32_zip_size_mb = Path(fp32_zip).stat().st_size / (1024.0 * 1024.0)\n",
    "print(f\"FP32 directory size: {fp32_dir_size_mb:.2f} MB\")\n",
    "print(f\"FP32 zip size: {fp32_zip_size_mb:.2f} MB\")\n",
    "files.download(fp32_zip)\n",
    "\n",
    "# Export clean INT8 inference package\n",
    "int8_dir = Path(\"speecht5_int8_deployment\")\n",
    "if int8_dir.exists():\n",
    "    shutil.rmtree(int8_dir)\n",
    "export_int8_deployment_package(finetuned_model, processor, int8_dir)\n",
    "int8_zip = shutil.make_archive(\"speecht5_int8_deployment\", \"zip\", root_dir=str(int8_dir))\n",
    "int8_dir_size_mb = directory_size_mb(int8_dir)\n",
    "int8_zip_size_mb = Path(int8_zip).stat().st_size / (1024.0 * 1024.0)\n",
    "print(f\"INT8 directory size: {int8_dir_size_mb:.2f} MB\")\n",
    "print(f\"INT8 zip size: {int8_zip_size_mb:.2f} MB\")\n",
    "files.download(int8_zip)\n",
    "\n",
    "# Speaker embedding for both modes\n",
    "torch.save(torch.tensor(speaker_embedding, dtype=torch.float32), \"speaker_embedding.pt\")\n",
    "files.download(\"speaker_embedding.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
